\section{\label{appendix1}Binary Networks}
We model a network of layer 2/3 neurons in the primary visual cortex with salt-and-pepper
organization, that receives input from excitatory cells in layer 4. 

We consider a binary network which can be studied fully analytically, enabling us to 
understand the mechanism of orientation selectivity in the limit where the 
connectivity is sparse and the average number of inputs, $K$, goes to infinity.
In this model we compare the mechanism for orientation selectivity in the
case where connectivity is solely dependent on distance, with the one where
the connectivity also depends on the difference in preferred orientation of 
the neurons.
The results presented here in the binary network without functional map also hold for more realistic conductance-based models. We refer to our previous study \cite{} for furter detalis. Conductance based model has the advantage that the relation between the firing and voltage statistics 
can be investigated here, an issue that cannot be addressed in the binary network.
%We also investigate in this model the finite sparseness and finite $K$ effects. 
 
The network consists of $N_E$ excitatory and $N_I$ inhibitory 
neurons organized on a ring with  period $\pi$\@. We denote neuron $i$ of 
population $A$ (with $A=E,I$ and $i=1,2,\ldots,N_A$) as neuron $(i,A)$. 
The position on the ring of neuron $(i,A)$ is $\phi_i^A=\pi i/N_A$.


\subsubsection*{The model}
In the binary network the neurons are modeled as binary units. They are 
updated sequentially. The state, $\sigma_i^A$, of neuron $(i,A)$ is updated, 
on average, once per time constant $\tau_A$\@. It is set to zero if, at the 
time of the update, its input, $u_i^A$, is below the threshold, $T_A$, 
and set to one otherwise. \\
 \\
{\noindent \bf Architecture:} The input, $u_i^A$, has three 
components: the feedforward input, $u_i^{A0}$, the recurrent excitatory input, 
$u_i^{AE}$, and the recurrent inhibitory feedback, $u_i^{AI}$\@. 

The recurrent excitation and inhibition are given by
\begin{equation}
u_i^{AB}(t)=\frac{J_{AB}}{\sqrt{K}}\sum_{j=1}^{N_B}C_{ij}^{AB}\sigma_j^B(t)
\qquad \qquad (B=E,I),
\end{equation}
where $J_{AB}/\sqrt{K}$, with $J_{AE}>0$ and $J_{AI}<0$, is the contribution 
of an active presynaptic cell to the recurrent input.
The connection matrix $C_{ij}^{AB}$ is randomly chosen: $C_{ij}^{AB}=1$ 
with probability $\frac{K}{N_B}[1+2p\cos2(\phi_i^A-\phi_j^B)]$ and 
$C_{ij}^{AB}=0$ otherwise. The modulation, $p$, $0\leq p \leq 1/2$, determines 
how much the connection probability depends on the distance between neurons 
on the ring. With this connection matrix a neuron receives, on average, input 
from $K$ excitatory and $K$ inhibitory neurons. 

The component, $u_i^{A0}$, represents the input from layer 4 neurons.
We assume that the neuron receives $cK$ feedforward inputs, which have a 
strength $J_{A0}/c\sqrt{K}$\@. We consider two cases: a cortex
with a functional orientation map and a cortex without such a map.

When there is a functional orientation map, the inputs from layer 4 are 
ordered. Cells receive inputs from layer 4 neurons that tend to respond 
maximally at roughly the same stimulus orientation. This results in a total 
feedforward input that has an orientation tuning comparable to that of
individual layer 4 neurons. Furthermore, neighboring layer 2/3 cells receive 
input from layer 4 neurons with similar preferred orientations (POs). 
We model the resulting feedforward input as
\begin{equation}
u_i^{A0}=\sqrt{K}J_{A0}m_0\left[1+\mu\cos 2 (\theta-\phi_i^A)\right],
\end{equation}
where $\theta$ is the stimulus orientation and $m_0$ is the average activity of the
projecting layer 4 neurons, which increases with the contrast. 
The variable $\mu$, $0<\mu<1$, determines how strongly the feedforward input is 
tuned with orientation. 
Note that the external input is maximal for $\theta=\phi_i^A$ and minimal for
$\theta=\phi_i^A+\pi/2$\@.

In the case without a functional orientation map, layer 2/3 neurons receive 
inputs from layer 4 neurons with POs that are randomly 
distributed. As a result, the total feedforward input is almost untuned, 
even if the response of layer 4 neurons is strongly tuned. In fact the tuning 
of the input is reduced by a factor $1/\sqrt{cK}$ compared to tuning of the 
projecting neurons. In this scenario the orientation for which the 
feedforward input is maximal is uncorrelated with the neuron's position. 
We model this by assuming that the external input into the neurons
satisfies
\begin{equation}
u_i^{A0}=\sqrt{K}J_{A0}m_0\left[1+\frac{\xi_A}{\sqrt{K}}
(v_i^A\cos 2\theta+w_i^A\sin 2\theta)\right],
\end{equation} 
Here $v_i^A$ and $w_i^A$ are random variables, independently drawn from a 
Gaussian distribution with mean 0 and variance 1. The average strength of the 
residual tuning in the external input is parametrized by $\xi_A$,  which depends 
on the sharpness of the tuning of the layer 4 neurons.

It is worth noting that the feedforward input 
can alternatively be written as
\begin{equation}
u_i^{A0}=J_{A0}m_0\left[\sqrt{K}+
\xi_A z_i^A\cos 2(\theta-\Delta_i^A)\right]
\end{equation}
where $z_i^A$ is a positive random variable drawn from the distribution 
$p(z)=ze^{-z^2/2}$ and $\Delta_i^A$ is an angle randomly chosen between 0 and 
$\pi$\@. 

\begin{table}
\begin{center}
\begin{tabular}{|l|c|c|c|}
\hline
 & & $A=E$ & $A=I$ \\ 
\hline
general & $T_A$ & 1 & 1 \\
 & $J_{A0}$ & 2 & 1 \\
 & $J_{AE}$ & 1 & 1 \\
 & $J_{AI}$ & 1.5 & 1 \\
 & $p$ & 0.5 & 0.5 \\
\hline
map & $\mu$ & 0.53 & 0.53 \\
\hline
nomap & $\xi_A$ & $\sqrt{15}$ & $\sqrt{15}$ \\
\hline
\end{tabular}
\end{center}
\caption{Parameters for the binary network.}
\label{bin-param:tab}
\end{table}

We study this model in the limit where the connectivity is sparse and $K$ goes 
to infinity, for $m_0=0.025$ (10\% contrast), $m_0=0.05$ (30\% contrast) 
and $m_0=0.075$ (100\% contrast). Unless specified otherwise, the other 
network parameters used are given in Table \ref{bin-param:tab}.

\subsubsection*{Analysis}

The analysis of the networks with and without orientation maps are very similar.
To avoid repeating almost the same arguments twice, it is convenient to 
analyze a network in which the feedforward input is given by 
\begin{equation}
u_i^{A0} = \sqrt{K}J_{A0}m_0\left[1+\mu\cos 2(\theta-\phi_{iA}^0)+
\frac{\xi_A}{\sqrt{K}}z_i^A\cos 2(\theta-\Delta_i^A)\right]
\end{equation}
Results for the network with orientation map are obtained by setting $\xi_A=0$,
while we obtain the results for the network without a map by taking $\mu=0$.

The analysis is similar to that of van Vreeswijk and Sompolinsky 
\cite{vanVreeswijk2005}, to which we refer for further details.\\
 \\
{\noindent \bf Coarse grained analysis:}
Because the strength of the synapses scales as $1/\sqrt{K}$, temporal 
fluctuations in the input do not vanish in the large $K$ limit. 
Furthermore, because  of the randomness of the connectivity and feedforward 
input, the time-average input for neurons of the same population is not the 
same, even for neurons which are very close. 
 
To analyze the network response we calculate the statistics of the different
variables for neurons $(i,A)$ with $\phi_i^A$ between $\phi$ and
$\phi+\delta\phi$ for a sufficiently small $\delta\phi$\@. Averages over this
subpopulation of variables $X_i^A$ are denoted by $\langle X_i^A
\rangle_{\phi,\delta\phi}$\@. For example $\langle u_i^A
\rangle_{\phi,\delta\phi}$ is the average input of neurons $(i,A)$ with 
$\phi<\phi_i^A<\phi+\delta\phi$\@. Because of the rotational and mirror 
symmetry of the system we have that, for a stimulus with orientation 
$\theta$, this average satisfies $\langle u_i^A(t)\rangle_{\phi,\delta\phi}=
u_A(\theta-\phi,t)$, where $u_A$ is symmetric. In fact for all variables $X$ of 
interest the average will satisfy $\langle X_i^A \rangle_{\phi,\delta \phi}=f_A
(\theta-\phi)$ for some symmetric function $f_A$. Thus it is sufficient to 
calculate the averages only for $\phi=0$ at different stimulus angles 
$\theta$\@. For notational convenience we will denote $\langle \cdot 
\rangle_{0,\delta \phi}$ by $\langle \cdot \rangle_{\delta \phi}$\@.

In the steady state, when the activities are constant, we can write the input into
neuron $(i,A)$, for a stimulus with orientation $\theta+\phi_i^A$ as
\begin{equation}
u_i^A(t)=u_{A}(\theta)+\Delta u_i^{A}(\theta)+\delta u_i^{A}(t),
\label{input_terms:eq}
\end{equation}
where $u_{A}$ is the population averaged input and the quenched disorder in $u_i^A$,
$\Delta u_i^{A}$, is the
difference between the averaged input into the neuron and the population 
average, $\Delta u_i^{A}\equiv \langle u_i^A-u_A\rangle_t$, where
$\langle\cdots\rangle_t$ denotes the average over time (the difference
between the time averaged value of $X$ and its population average is 
called the {\it quenched disorder} in $X$\@).
Finally, $\delta u_i^{A}(t)$ represents the temporal fluctuation in the input.

Because $\Delta u_i^A$ and $\delta u_i^{A}(t)$ are composed of many small
contributions, they have Gaussian statistics.
Thus we alternatively can write
\begin{equation}
u_i^A(t)=u_{A}(\theta)+\sqrt{\gamma_A(\theta)}x_i^A(\theta)+
\sqrt{\alpha_A(\theta)-\gamma_A(\theta)}y_i^A(t),
\label{input_terms1:eq}
\end{equation}
where $\gamma_A$ and $\alpha_A-\gamma_A$ are the variance of the quenched disorder
and the temporal fluctuations respectively, and $x_i^A$ and $y_i^A$ are 
Gaussian random variables with mean 0 and variance $\langle [x_i^A]^2\rangle_{\delta \phi}=
\langle [y_i^A(t)]^2\rangle_{\delta \phi}=1$\@. Furthermore, for sufficiently large 
$t-t^\prime$, $y_i^A(t)$ and $y_i^A(t^\prime)$ are uncorrelated,
$\langle y_i^A(t)y_i^A(t^\prime)\rangle_{\delta \phi}\rightarrow 0$, for
$|t-t^\prime|\rightarrow\infty$\@.

In the steady state, the activity of neuron $(i,A)$, $m_i^A=
\langle \sigma_i^A\rangle_t$, is equal to the probability that the
fluctuations bring the input above threshold,
\begin{equation}
m_i^A=\int\!\frac{dy}{\sqrt{2\pi}}\,e^{-y^2/2}
\Theta \left(u_A+\sqrt{\gamma_A}x_i^A+\sqrt{\alpha_A-\gamma_A}y-T_A\right),
\end{equation}
where $\Theta$ is the Heaviside function, $\Theta(x)=1$ for $x>0$ and
$\Theta(x)=0$ for $x<0$\@. This can be written as
\begin{equation}
m_i^A=H\left(\frac{T_A-u_A-\sqrt{\gamma_A}x_i^A}{ \sqrt{\alpha_A-\gamma_A}}
\right).
\label{indiv-rate:eq}
\end{equation}
Here $H$ denotes the cumulative Gaussian distribution,
$H(x)=\frac{1}{\sqrt{2\pi}}\int_x^\infty\!dy\,e^{-y^2/2}$\@.

Because we know the distribution of $x_i^A$, Eqn.~(\ref{indiv-rate:eq}) 
determines, for a stimulus with given contrast and orientation, the distribution 
of the activities for neurons $(i,A)$ with $\phi<\phi_i^A<\phi+\delta\phi$, 
if we know $u_A$, $\alpha_A$ and $\gamma_A$\@. 

However, we also need to take into account that the random variable $x_i^A$
changes when the stimulus parameters are changed. For example, if
$\theta$ is changed to $\theta^\prime$, $x_i^A$ changes to
$x_i^{\prime A}$ and $x_i^{\prime A}$ should be close to
$x_i^A$ if $\theta-\theta^\prime$ is small. Thus the input statistics of population $A$ is fully known if we have a
description of $u_A$ and $\alpha_A$ for any $m_0$ and $\theta$ and if we know the
correlation in the quenched disorder for any pair of stimuli,
$(m_0,\theta)$ and $(m_0^\prime,\theta^\prime)$.  Below we will sketch
the analysis to derive these parameters.

A convenient feature of the binary neuron model is that the average activity,
$m_A\equiv\langle m_i^A\rangle_{\delta\phi}$, given by
\begin{eqnarray}
m_A(\theta)=&&\int\!\frac{dx}{\sqrt{2\pi}}\,e^{-x^2/2}
H\left(\frac{T_A-u_A(\theta)-\sqrt{\gamma_A(\theta)}x}
{ \sqrt{\alpha_A(\theta)-\gamma_A(\theta)}}\right) \nonumber \\
=&& H\left(\frac{T_A-u_A(\theta)}{\sqrt{\alpha_A(\theta)}}\right),
\label{meanrate:eq}
\end{eqnarray}
is independent of $\gamma_A$\@. Thus if we can express $u_A$ and $\alpha_A$ in 
the mean activities, $m_B$, this together with Eqn.~(\ref{meanrate:eq}) 
determines these quantities.\\
 \\
{\bf Population averaged inputs:}
The term $u_A(\theta)$ in Eqn.~(\ref{input_terms:eq}) is the average 
of the input $u_i^A$, for neurons 
in population $A$ with $0<\phi_i^A<\delta \phi$, when the stimulus orientation is $\theta$.
It is given by 
$u_A(\theta)=\sum_{B=0,E,I}u_{AB}(\theta)$ where
\begin{equation}
u_{A0}(\theta)=\langle u_i^{A0}\rangle_{\delta\phi}= 
\sqrt{K}J_{A0}m_0[1+\mu\cos 2\theta],
\label{uA0:eq}
\end{equation}
while for $A=E,I$
\begin{eqnarray}
u_{AB}(\theta) & = & \langle u_i^{AB}\rangle_{\delta\phi}  =  \frac{J_{AB}}{\sqrt{K}} \sum_{j=1}^{N_B}
\langle C_{ij}^{AB}\rangle_{\delta\phi}m_B(\theta-\phi_j^B)
\nonumber \\
& = & \sqrt{K}J_{AB}\left[ m_B^{(0)}+p\,m_B^{(1)}\cos 2\theta\right].
\label{uAB:eq}
\end{eqnarray}
Here $m_A^{(k)}$ is the $k$th Fourier moment of $m_A$, $m_A(\theta)=
\sum_{k=0}^\infty m_A^{(k)}\cos 2k\theta$\@.\\
 \\
{\bf Equal time Fluctuations of the Inputs:}
Because the random connection matrices and feedforward inputs 
are independent, the fluctuations in the different components of the 
input are independent so that we can write for $\alpha_A$,
$\alpha_A(\theta)=\sum_{B=0,E,I}\,\alpha_{AB}(\theta)$,
where $\alpha_{AB}(\theta)=
\langle [u_i^{AB}(t)-u_{AB}(\theta)]^2\rangle_{\delta\phi}$\@.

For the feedforward input we have
\begin{equation}  
\alpha_{A0}(\theta) = [J_{A0}\xi_A m_0]^2
\langle[z_i^A\cos 2(\theta-\Delta_i^A)]^2\rangle
= [J_{A0}\xi_A m_0]^2.
\label{alA0:eq}
\end{equation}
Here we have used that $z_i^A$ and $\Delta_i^A$ are independent, 
$\langle [z_i^A]^2\rangle=2$ and $\langle \cos^2 2(\theta-\Delta_i^A)\rangle=1/2$\@.

For the variance of the feedback, $\alpha_{AB}$, with $B=E,I$, we have
(see \cite{vanVreeswijk2005})
\begin{eqnarray} 
\alpha_{AB}(\theta) & = &\frac{J_{AB}^2}{K}\sum_j
\langle\sum_j C^{AB}_{ij}\rangle_{\delta\phi} m_B(\theta-\phi_j^B)
\nonumber \\
 & = & J_{AB}^2[m_B^{(0)}+p\,m_B^{(1)}\cos 2\theta].
\label{alAB:eq}
\end{eqnarray}

{\noindent \bf Balanced solution:}
In the steady state the averaged activities $m_A$ are given by Eqn.~(\ref
{meanrate:eq}), where by combining Eqns.~(\ref{uA0:eq}) and (\ref{uAB:eq}), we can 
write $u_A$ as $u_A(\theta)= u_A^{(0)}+u_A^{(1)}\cos 2\theta$, while with
Eqns.~(\ref{alA0:eq}) and (\ref{alAB:eq}) we can write $\alpha_A$ as 
$\alpha_A(\theta)= \alpha_A^{(0)}+\alpha_A^{(1)}\cos 2\theta$.
The equal time fluctuations $\alpha_A$ are of order 1, while $u_A$ is of order 
$\sqrt{K}$, unless the leading terms in $u_A^{(0)}$ and $u_A^{(1)}$ cancel.
In the large $K$ limit this means that, when this cancellation does not take place,
 $m_A$ goes to either $m_A=0$ or $m_A=1$,
depending on the sign of $u_A$. 
Therefore the only way in which the system can have low, but non-zero, activities is 
if in the leading order of both $u_A^{(0)}$ and $u_A^{(1)}$, the recurrent 
inhibitory input cancels the total excitatory input.

Imposing this requirement for both populations leads to the balanced
solution where, up to a correction term of order $1/\sqrt{K}$, $m_A$ satisfies
$m_A^{(0)}=A_Am_0$, and $m_A^{(1)}=\frac{\mu}{p}A_Am_0$, where
$A_E=\frac{J_{I0}J_{EI}-J_{E0}J_{II}}{J_{EE}J_{II}-J_{EI}J_{IE}}$ and
$A_I=\frac{J_{E0}J_{IE}-J_{I0}J_{EE}}{J_{EE}J_{II}-J_{EI}J_{IE}}$\@.

This determines $\alpha_A$ in the large $K$ limit (Eqn.~(\ref{alAB:eq})). 
But $u_A$ now depends on 
the $1/\sqrt{K}$ corrections of the activity and remains to be determined. 
The average, $u_A^{(0)}$, and modulation, $u_A^{(1)}$, are determined by

\begin{equation}
m_A^{(0)}=\frac{1}{\pi}\int_0^\pi\!d\theta\,
H\left(\frac{T_A-u_A^{(0)}-u_A^{(1)}\cos 2\theta}{\sqrt{\alpha_A(\theta)}}
\right)
\end{equation}
and
\begin{equation}
m_A^{(1)}=\frac{2}{\pi}\int_0^\pi\!d\theta\,
H\left(\frac{T_A-u_A^{(0)}-u_A^{(1)}\cos 2\theta}{\sqrt{\alpha_A(\theta)}}
\right)\cos 2\theta.
\end{equation}

\noindent{\bf Statistics of the quenched disorder:}
The activity $m_i^A(\theta)$ satisfies
\begin{equation}
m_i^A(\theta)=H\left(\frac{T_A-u_A(\theta)-\sqrt{\gamma_A(\theta)}
x_i^A(\theta)}{\sqrt{\alpha_A(\theta)-\gamma_A(\theta)}}\right).
\label{ind-rate:eq}
\end{equation}
Since $x_i^A(\theta)$ is drawn from a Gaussian with mean 0 and variance 1,
this completely determines the distribution of activitie, $\Pr[m_i^A(\theta)]$, 
 if $\gamma_A$ is known. However, to calculate the joint distribution,
$\Pr[m_i^A(\theta_1),m_i^A(\theta_2),\ldots,m_i^A(\theta_n)]$ of the activities
of a neuron, for stimulus angles $\theta_1,\theta_2,\ldots,\theta_n$, we 
need to know the joint statistics of disorder variables, $x_i^A(\theta_1),
x_i^A(\theta_2),\ldots,x_i^A(\theta_n)$\@. Luckily these are Gaussian random
variables, so that their joint statistics are fully determined by the 
cross-correlations, $\langle(x_i^A(\theta_k)x_i^A(\theta_l)\rangle$\@.
We will now determine these correlations.

It is convenient to write the correlations between $x_i^A$ at angle 
$\theta+\Delta$ and $x_i^A$ at angle $\theta-\Delta$ as
$\langle x_i^A(\theta+\Delta)x_i^A(\theta-\Delta)\rangle=
\frac{\beta_A(\theta,\Delta)}
{\sqrt{\gamma_A(\theta+\Delta)\gamma_A(\theta-\Delta)}}$\@. Since
$\langle [x_i^A(\theta)]^2\rangle=1$, we have that 
$\beta_A(\theta,0)=\gamma_A(\theta)$\@.

We introduce a new variable, $q_A$ defined by $q_A(\theta,\Delta)\equiv
\langle m_i^A(\theta+\Delta)m_i^A(\theta-\Delta)\rangle_{\delta\phi}$\@.
This is the joint probability of a neuron being in the active state both
for a stimulus at $\theta+\Delta$ and at $\theta-\Delta$.
It can be calculated using Eqn.~(\ref{ind-rate:eq}), by averaging over 
the correlated Gaussian variables $x_i^A(\theta+\Delta)$ and 
$x_i^A(\theta-\Delta)$\@. After some algebra one obtains
\begin{eqnarray}
q_{A}(\theta,\Delta) & = &
\int\!Dx\, H\left(\frac{T_A-u_A^+-\sqrt{\beta_A}x}
{\sqrt{\alpha_A^+-\beta_A}}\right) \nonumber \\
&\qquad& \times H\left(\frac{T_A-u_A^--\sqrt{\beta_A}x}
{\sqrt{\alpha_A^--\beta_A}}\right)
\label{qA:eq}
\end{eqnarray}
where we used the abbreviations, $u_A^\pm=u_{A}(\theta\pm\Delta)$,
$\alpha_A^\pm=\alpha_{A}(\theta\pm\Delta)$ and
$\beta_A=\beta_{A}(\theta,\Delta)$\@.

Following the same logic as for the fluctuations $\alpha_A$, we can write for
the correlations $\beta_{A}$, $\beta_{A}(\theta,\Delta)=
\sum_{B=0,E,I}\beta_{AB}(\theta,\Delta)$, where 
$\beta_{AB}(\theta,\Delta)=
\langle u_i^{AB}(t)u_i^{AB}(t^\prime)\rangle_{\delta\phi}-
\langle u_i^{AB}(t)\rangle_{\delta\phi}
\langle u_i^{AB}(t^\prime)\rangle_{\delta\phi}$ is the 
contribution to the input correlation due to input from population $B$\@.

The contribution to this correlation from the external input is given by
\begin{eqnarray}  
\beta_{A0}(\theta,\Delta) & = &
[J_{A0}\xi_A m_0]^2\langle[z_i^A]^2\cos 2(\theta+\Delta-\Delta_i^A)
\cos 2(\theta-\Delta-\Delta_i^A)\rangle_{\delta\phi} \nonumber \\
 & = &
[J_{A0}\xi_A m_0]^2\langle[z_i^A]^2\rangle_{\delta\phi}
\langle \cos 2(\theta+\Delta-\Delta_i^A)
\cos 2(\theta-\Delta-\Delta_i^A)\rangle_{\delta\phi} \nonumber \\
 & = & [J_{A0}\xi_A m_0]^2 \cos 4\Delta.
\end{eqnarray}

The input correlations due to the two feedback components, $\beta_{AB}$,
with $B=E,I$, depend on $q_{A}$ and is given by
(see \cite{vanVreeswijk2005})
\begin{equation} 
\beta_{AB}(\theta,\Delta)=
J_{AB}^2[q_{B}^{(0)}(\Delta)+pq_{B}^{(1)}(\Delta)\cos 2\theta],
\label{betaA:eq}
\end{equation}
where $q_{B}^{(k)}$ is the $k$th Fourier component in the variable $\theta$ of
$q_{B}$, $q_{B}(\theta,\Delta)=\sum_k q_{B}^{(k)}(\Delta)\cos 2k\theta$\@.

This expresses $\beta_{A}(\theta,\Delta)$ in $q_{A}^{(0)}(\Delta)$ and
$q_{A}^{(1)}(\Delta)$\@. Self-consistent solutions for these are obtained 
by imposing $q_{A}^{(0)}(\Delta)=\frac{1}{\pi}\int_0^\pi\!d\phi\,
q_{A}(\theta,\Delta)$ and $q_{A}^{(1)}(\Delta)=\frac{2}{\pi}
\int_0^\pi\!d\phi\,q_{A}(\theta,\Delta)\cos 2\theta$ where 
$q_{A}(\theta,\Delta)$ is given by Eqn.~(\ref{qA:eq}).

Extending these results to the case where the feedforward activity, $m_0$, is also 
changed is straightforward:
The population averaged input $u_A$ and input variance $\alpha_A$ are now
functions of $m_0$ and $\theta$, and are calculated as before. 
For the correlations we have to consider 2 stimuli specified by variables
($m_0^+$,$\theta+\Delta$) and ($m_0^-$,$\theta-\Delta$) respectively.  
The correlations in the total input are denoted by 
$\beta_A(m_0^+,m_0^-,\theta,\Delta)$, while for the correlations in the 
activity we write $q_A(m_0^+,m_0^-,\theta,\Delta)$\@.
The corelation $\beta_A$ depends on $q_A$ as
$\beta_A(m_0^+,m_0^-,\theta,\Delta)=[J_{0A}\xi_A]^2m_0^+m_0^-\cos 4\Delta+
\sum_{B=E,I}J_{AB}^2[q_B^{(0)}(m_0^+,m_0^-,\Delta)+
pq_B^{(1)}(m_0^+,m_0^-,\Delta)\cos 2\theta]$, where 
$q_A^{(k)}(m_0^+,m_0^-,\Delta)$ is the $k$th Fourier moment in $\theta$
of $q_A(m_0^+,m_0^-,\theta,\Delta)$\@. $q_A$ is still given by
Eqn.~(\ref{qA:eq}), except that now $u_A^\pm=u_A(m_0^\pm,\theta\pm\Delta)$,
$\alpha_A^\pm=\alpha_A(m_0^\pm,\theta\pm\Delta)$ and 
$\beta_A=\beta_A(m_0^+,m_0^-,\theta,\Delta)$\@.
A self-consistency requirement equivalent to that given above for 
$m_0^\pm=m_0$  determines $\beta_A$.\\
 \\
{\noindent \bf Symmetries:}
The connection probabilities are even functions of the difference in 
positions, $\phi_i^A-\phi_j^B$ and the external input is 
symmetric in $\theta-\phi_i^A$\@. This implies that 
$\beta_{A}(m_0,m_0^\prime,\theta,\Delta)=
\beta_{A}(m_0,m_0^\prime,-\theta,-\Delta)$.
As shown above, $\beta_{A}(m_0,m_0^\prime,\theta,\Delta)=
\beta_{A}(m_0,m_0^\prime,-\theta,\Delta)$\@. Together these two symmetries also imply that
$\beta_{A}(m_0,m_0^\prime,\theta,\Delta)=
\beta_{A}(m_0,m_0^\prime,\theta,-\Delta)$\@. Furthermore, under the 
transformation $(\theta,\Delta)\rightarrow (\theta+\pi/2,\Delta-\pi/2)$ the two
input orientations, $\theta_1=\theta+\Delta$ and $\theta_2=\theta-\Delta$, 
transform to $\theta_1 \rightarrow \theta_1$ and 
$\theta_2\rightarrow\theta_2+\pi$\@. With the $\pi$ periodicity of the system 
this implies that $\beta_{A}(m_0,m_0^\prime,\theta,\Delta)=
\beta_{A}(m_0,m_0^\prime,\theta+\pi/2,\Delta-\pi/2)$\@. Finally, if we make the
change, $(m_0,\theta)\rightarrow(m_0^\prime,\theta^\prime)$ and 
$(m_0^\prime,\theta^\prime)\rightarrow(m_0,\theta)$, the correlations are not 
changed either. This implies that 
$\beta_{A}(m_0,m_0^\prime,\theta,\Delta)=\beta_{A}(m_0^\prime,m_0,\theta,-\Delta)$\@.

Taking these symmetries into account we can write 
$\beta_{A}(m_0,m_0^\prime,\theta,\Delta)=\beta_{A}^{(0)}
(m_0,m_0^\prime,\Delta)+$\\$\beta_{A}^{(1)}(m_0,m_0^\prime,\Delta)\cos 2\theta$ as
\begin{eqnarray}
\beta_{A}(m_0,m_0^\prime,\theta,\Delta) &=& \sum_{n=0}^\infty \beta_{A}^{(0,2n)}(m_0,m_0^\prime)\cos 4n \Delta
\nonumber \\
&\quad& + \beta_{A}^{(1,2n+1)}(m_0,m_0^\prime)\cos 2\theta \nonumber \\
&\qquad& \qquad \times \cos 2(2n+1) \Delta,
\end{eqnarray}
where $\beta_{A}^{(0,n)}$ and $\beta_{A}^{(1,n)}$ are the $n$th Fourier
components in $\Delta$ of $\beta_{A}^{(0)}$ and $\beta_{A}^{(1)}$
respectively and $\beta^{(n,m)}(m_0,m_0^\prime)=
\beta^{(n,m)}(m_0^\prime,m_0)$\@.

\noindent{\bf The solution in the cases with and without map:}
So far we have considered the solution in the general case. What does this
imply for the network without a map ($\mu=0$) and the case with a map ($\xi_A=0$)?

When there is no map, 
$m_A^{(1)}=u_A^{(1)}=0$ and therefore, from Eqns.~(\ref{alA0:eq}) and 
(\ref{alAB:eq});
$\alpha_A^{(1)}=0$\@. If we now consider Eqns.~(\ref{qA:eq}) and 
(\ref{betaA:eq}) we see that the fact that $u_A$ and $\alpha_A$ do not depend 
on $\theta$ implies that $q_A^{(1)}(\Delta)=\beta_A^{(1)}(\Delta)=0$\@. 
Therefore, $q_A$ and $\beta_A$ do not depend on $\theta$, but only 
on $\Delta$\@. Notice furthermore
that the factor $p$, which determines how strongly the probability of 
connections is modulated with distance, only enters into the expressions for
$u_A^{(1)}$, $m_A^{(1)}$, $q_A^{(1)}$ and $\beta_A^{(1)}$\@. Since these are 
all 0 when $\mu=0$, in a network a without functional map the solution is
independent of $p$\@.

When there is a map, none of these simplifications apply. It is 
however worth noting that in this case, since $m_A^{(0)}$ and $m_A^{(1)}$
both grow proportionally with $m_0$, their ratio in independent of $m_0$\@.
This means that the Circular Variance (\CircVar, see below) of the population
averaged response satisfies $\CircVar=1-\mu/p$,and is independent of contrast.

\subsubsection*{Generating tuning curves}
Setting $\sqrt{\gamma_A(\theta)}x_i^A(\theta)$ to $\Delta u_i^A$, we can
write Eqn. (\ref{ind-rate:eq}) as
\begin{equation}
m_i^A(m_0,\theta)=H\left(\frac{T_A-u_A(m_0,\theta)-\Delta u_i^A(m_0,\theta)}
{\sqrt{\alpha_A(m_0,\theta)-\beta_A(m_0,m_0,\theta,0)}}\right),
\label{indi-rate:eq}
\end{equation}
for a feedforward input with mean activity $m_0$ and stimulus angle $\theta$\@.
We have explained how to calculate $u_A$, $\alpha_A$ and $\beta_A$ and that
$\Delta u_i^A(m_0,\theta)$ is a Gaussian random field with
mean 0 and correlations $\langle \Delta u_i^A(m_0,\theta+\Delta)
\Delta u_i^A(m_0^\prime,\theta-\Delta)\rangle=
\beta_{A}(m_0,m_0^\prime,\theta,\Delta)$\@. Thus the statistics of the input, 
and hence of the activity, is fully specified. 

Unfortunately, due to the 
non-linear relation between input and activity, Eqn.~(\ref{indi-rate:eq}),
it is not straightforward to translate this knowledge into meaningful
statements about the properties of the activity of single cells in population 
$A$, such as the distribution of circular variances of the tuning curves at
a given contrast, or how the tuning curves are modified as the contrast is 
changed. 

We use the following approach: we generate the activity of sample neurons 
at different $m_0$ and $\theta$ that are consistent with the calculated 
statistics, and use averaging over these samples to determine the desired 
properties. The method to generate these sample outputs is as follows.

Since $\Delta u_i^{A}$ is $\pi$-periodic in $\theta$, it can be
written as
\begin{eqnarray}
\Delta u_i^{A}(m_0,\theta) &=& \sum_{n=0}^\infty V^{(n)}(m_0)\cos 2n\theta \nonumber \\
&\qquad& + W^{(n)}(m_0)\sin 2n\theta
\end{eqnarray}
where $V^{(n)}$ and $W^{(n)}$ are Gaussian random variables with a mean of 0,
whose correlations should be chosen such that 
$\langle \Delta u_i^{A}(m_0,\theta+\Delta)
\Delta u_i^{A}(m_0^\prime,\theta-\Delta)\rangle=
\beta_A(m_0,m_0^\prime,\theta,\Delta)$\@. 

After some straightforward, but tedious algebra one finds that
the correlations have to satisfy 
$\langle V^{(n)}(m_0)W^{(m)}(m_0^\prime)\rangle=0$ and
$\langle V^{(n)}(m_0)V^{(m)}(m_0^\prime)\rangle=
 \langle W^{(n)}(m_0)W^{(m)}(m_0^\prime)\rangle=$\\$
\frac{1}{2}(1+\delta_{n,m})\beta_{A}^{(|n-m|,n+m)}(m_0,m_0^\prime)$\@.
Here $\delta_{n,m}$ is the Kronecker delta, $\delta_{n,m}=1$ for $n=m$ and
$\delta_{n,m}=0$ for $n\neq m$\@.

In principle one would need infinitely many random variables $V^{(n)}$ and 
$W^{(n)}$, but in practice one can get a very good approximation by using a 
rather small number, since $\langle [V^{(n)}(m_0)]^2\rangle=
\langle [W^{(n)}(m_0)]^2\rangle=\beta_{A}^{(0,2n)}(m_0,m_0)$, which rapidly 
decreases as $n$ is increased. Thus the 
amplitude of the higher frequency components in the quenched disorder is 
increasingly small. As a result, setting terms with $n$ larger than some cut 
off $n_0$ has no noticeable effect on the output statistics.
For the parameters we use in this paper we can take $n_0$ as low as $n_0=5$\@.

To generate tuning curves for a neuron of population $A$, for 
$k_0$ contrasts, corresponding to input activities $m_0=m_{0k}$, for 
$k=1,2,\ldots,k_0$ we determine $u_A^{(n)}(m_{0k})$ and 
$\alpha_{A}^{(n)}(m_{0k})$ for each $k$ and $n=0,1$ and calculate
$\beta_A^{(0,2n)}(m_{0k},m_{0l})$ and 
$\beta_A^{(1,2n+1)}(m_{0k},m_{0l})$ for $k,l \in \{1,2,\ldots,k_0\}$
and $n\in \{0.1.\ldots,n_0\}$\@. Next we construct a $N\times N$ correlation 
matrix, $C$, where $N=k_0(n_0+1)$, in which the elements, $C_{i,j}$ satisfy
$C_{k_0n+k,k_0m+l}=\frac{1}{2}(1+\delta_{n,m})\beta_A^{(|n-m|.n+m)}
(m_{0k},m_{0l})$ if $|n-m|\leq 1$ and $C_{k_0n+k,k_0m+l}=0$ otherwise. 
We use Cholesky decomposition \cite{Horn1985} to find the lower triangular matrix 
$L$ which satisfies $LL^{\sf T}=C$\@. We construct two $N$ dimensional vectors, 
$\vec{x}$ and $\vec{y}$ whose elements are independently drawn from a Gaussian 
distribution with mean 0 and variance 1.
From these we calculate the vectors $\vec{v}=L\vec{x}$ and $\vec{w}=L\vec{y}$\@.
Using this procedure we have that, on average $\langle\vec{v}\vec{v}^{\sf T}
\rangle=\langle\vec{w}\vec{w}^{\sf T}\rangle=C$ and $\langle\vec{v}
\vec{w}^{\sf T}\rangle=0$\@.
Therefore, if we set $V_k^{(n)}=v_{k_0n+k}$ and $W_k^{(n)}=w_{k_0n+k}$, the 
time averaged quenched disorder, $\Delta u_i^A(m_0,\theta)$, will have the desired 
statistics. 

Using Eqn.~(\ref{indi-rate:eq}) we can now calculate the neuronal output for
different angles $\theta$ and different input levels $m_0$, to generate the
orientation tuning curves for a sample neuron at different contrasts.
More samples can be produced by applying this algorithms to many sets
of vectors, $\vec{x}$ and $\vec{y}$, drawn independently.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../main"
%%% End:
